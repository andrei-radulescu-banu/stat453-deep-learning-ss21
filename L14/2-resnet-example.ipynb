{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L14/2-resnet-example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark\n",
        "!pip install colab-env --upgrade\n",
        "import colab_env\n",
        "colab_env.envvar_handler.add_env(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\", overwrite=True)\n",
        "!git clone https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21.git\n",
        "import sys, os\n",
        "sys.path.append(\"/content/stat453-deep-learning-ss21/L14\")"
      ],
      "metadata": {
        "id": "fW03oG9syxdM",
        "outputId": "76e24bfc-6890-45df-ae73-80492ab8ef87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting watermark\n",
            "  Downloading watermark-2.3.1-py2.py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from watermark) (7.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from watermark) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->watermark) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->watermark) (4.1.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (5.1.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 19.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->watermark) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->watermark) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->watermark) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->watermark) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->watermark) (0.7.0)\n",
            "Installing collected packages: jedi, watermark\n",
            "Successfully installed jedi-0.18.2 watermark-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colab-env\n",
            "  Downloading colab-env-0.2.0.tar.gz (4.7 kB)\n",
            "Collecting python-dotenv<1.0,>=0.10.0\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: colab-env\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-env: filename=colab_env-0.2.0-py3-none-any.whl size=3837 sha256=352b03924e41f2ff3dacd56c042819d2304f2974e22ac73831ceaf66ae5cdc21\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ca/e8/3d25b6abb4ac719ecb9e837bb75f2a9b980430005fb12a9107\n",
            "Successfully built colab-env\n",
            "Installing collected packages: python-dotenv, colab-env\n",
            "Successfully installed colab-env-0.2.0 python-dotenv-0.21.0\n",
            "Mounted at /content/gdrive\n",
            "Cloning into 'stat453-deep-learning-ss21'...\n",
            "remote: Enumerating objects: 1215, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 1215 (delta 97), reused 4 (delta 4), pack-reused 1050\u001b[K\n",
            "Receiving objects: 100% (1215/1215), 115.67 MiB | 16.45 MiB/s, done.\n",
            "Resolving deltas: 100% (201/201), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LS5LaOlywyI"
      },
      "source": [
        "STAT 453: Deep Learning (Spring 2021)  \n",
        "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
        "\n",
        "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2021/  \n",
        "GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss21\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KYJXF_5YywyK",
        "outputId": "7507103a-5a27-4f31-cc9d-de8a9294a532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sebastian Raschka\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.15\n",
            "IPython version      : 7.9.0\n",
            "\n",
            "torch: 1.12.1+cu113\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%load_ext watermark\n",
        "%watermark -a 'Sebastian Raschka' -v -p torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8QhDeLywyM"
      },
      "source": [
        "- Runs on CPU or GPU (if available)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jw8qiXaywyM"
      },
      "source": [
        "# A Convolutional ResNet and Residual Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIfjURXVywyO"
      },
      "source": [
        "Please note that this example does not implement a really deep ResNet as described in literature but rather illustrates how the residual blocks described in He et al. [1] can be implemented in PyTorch.\n",
        "\n",
        "- [1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RJ4gdxwywyP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpBpKhyJywyP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b5YBCjBywyQ"
      },
      "source": [
        "## Settings and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhOAxdMywyQ",
        "outputId": "d6d16e65-3b03-4051-fd47-627843ad8194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "##########################\n",
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "random_seed = 123\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# Architecture\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "##########################\n",
        "### MNIST DATASET\n",
        "##########################\n",
        "\n",
        "# Note transforms.ToTensor() scales input images\n",
        "# to 0-1 range\n",
        "train_dataset = datasets.MNIST(root='data', \n",
        "                               train=True, \n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data', \n",
        "                              train=False, \n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=batch_size, \n",
        "                         shuffle=False)\n",
        "\n",
        "# Checking the dataset\n",
        "for images, labels in train_loader:  \n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUm6fzREywyR"
      },
      "source": [
        "## ResNet with identity blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DgNLevvywyS"
      },
      "source": [
        "The following code implements the residual blocks with skip connections such that the input passed via the shortcut matches the dimensions of the main path's output, which allows the network to learn identity functions. Such a residual block is illustrated below:\n",
        "\n",
        "![](https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L14/2-resnet-ex/resnet-ex-1-1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "S4OmIiQ_ywyS"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "### MODEL\n",
        "##########################\n",
        "\n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        #########################\n",
        "        ### 1st residual block\n",
        "        #########################\n",
        "        \n",
        "        self.block_1 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=1,\n",
        "                                out_channels=4,\n",
        "                                kernel_size=(1, 1),\n",
        "                                stride=(1, 1),\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm2d(4),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "                torch.nn.Conv2d(in_channels=4,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.BatchNorm2d(1)\n",
        "        )\n",
        "        \n",
        "        self.block_2 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=1,\n",
        "                                out_channels=4,\n",
        "                                kernel_size=(1, 1),\n",
        "                                stride=(1, 1),\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm2d(4),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "                torch.nn.Conv2d(in_channels=4,\n",
        "                                out_channels=1,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.BatchNorm2d(1)\n",
        "        )\n",
        "\n",
        "        #########################\n",
        "        ### Fully connected\n",
        "        #########################        \n",
        "        self.linear_1 = torch.nn.Linear(1*28*28, num_classes)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #########################\n",
        "        ### 1st residual block\n",
        "        #########################\n",
        "        shortcut = x\n",
        "        x = self.block_1(x)\n",
        "        x = torch.nn.functional.relu(x + shortcut)\n",
        "        \n",
        "        #########################\n",
        "        ### 2nd residual block\n",
        "        #########################\n",
        "        shortcut = x\n",
        "        x = self.block_2(x)\n",
        "        x = torch.nn.functional.relu(x + shortcut)\n",
        "        \n",
        "        #########################\n",
        "        ### Fully connected\n",
        "        #########################\n",
        "        logits = self.linear_1(x.view(-1,  1*28*28))\n",
        "        return logits\n",
        "\n",
        "    \n",
        "torch.manual_seed(random_seed)\n",
        "model = ConvNet(num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tZ-33cGywyT"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrpK7uzTywyU",
        "outputId": "e472e1ee-73c1-4b6c-dfb9-5106eeaff6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/010 | Batch 000/469 | Cost: 2.6938\n",
            "Epoch: 001/010 | Batch 250/469 | Cost: 0.3444\n",
            "Epoch: 001/010 training accuracy: 91.21%\n",
            "Time elapsed: 0.29 min\n",
            "Epoch: 002/010 | Batch 000/469 | Cost: 0.4317\n",
            "Epoch: 002/010 | Batch 250/469 | Cost: 0.4750\n",
            "Epoch: 002/010 training accuracy: 92.69%\n",
            "Time elapsed: 0.57 min\n",
            "Epoch: 003/010 | Batch 000/469 | Cost: 0.2430\n",
            "Epoch: 003/010 | Batch 250/469 | Cost: 0.2722\n",
            "Epoch: 003/010 training accuracy: 92.56%\n",
            "Time elapsed: 0.86 min\n",
            "Epoch: 004/010 | Batch 000/469 | Cost: 0.2547\n",
            "Epoch: 004/010 | Batch 250/469 | Cost: 0.2330\n",
            "Epoch: 004/010 training accuracy: 92.79%\n",
            "Time elapsed: 1.15 min\n",
            "Epoch: 005/010 | Batch 000/469 | Cost: 0.0948\n",
            "Epoch: 005/010 | Batch 250/469 | Cost: 0.2342\n",
            "Epoch: 005/010 training accuracy: 93.24%\n",
            "Time elapsed: 1.43 min\n",
            "Epoch: 006/010 | Batch 000/469 | Cost: 0.2508\n",
            "Epoch: 006/010 | Batch 250/469 | Cost: 0.2552\n",
            "Epoch: 006/010 training accuracy: 93.11%\n",
            "Time elapsed: 1.72 min\n",
            "Epoch: 007/010 | Batch 000/469 | Cost: 0.0941\n",
            "Epoch: 007/010 | Batch 250/469 | Cost: 0.3898\n",
            "Epoch: 007/010 training accuracy: 93.14%\n",
            "Time elapsed: 2.00 min\n",
            "Epoch: 008/010 | Batch 000/469 | Cost: 0.2865\n",
            "Epoch: 008/010 | Batch 250/469 | Cost: 0.1691\n",
            "Epoch: 008/010 training accuracy: 93.73%\n",
            "Time elapsed: 2.29 min\n",
            "Epoch: 009/010 | Batch 000/469 | Cost: 0.3685\n",
            "Epoch: 009/010 | Batch 250/469 | Cost: 0.1544\n",
            "Epoch: 009/010 training accuracy: 92.88%\n",
            "Time elapsed: 2.58 min\n",
            "Epoch: 010/010 | Batch 000/469 | Cost: 0.3586\n",
            "Epoch: 010/010 | Batch 250/469 | Cost: 0.4787\n",
            "Epoch: 010/010 training accuracy: 93.65%\n",
            "Time elapsed: 2.87 min\n",
            "Total Training Time: 2.87 min\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, data_loader):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    for i, (features, targets) in enumerate(data_loader):            \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        logits = model(features)\n",
        "        _, predicted_labels = torch.max(logits, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model = model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "        \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(features)\n",
        "        cost = torch.nn.functional.cross_entropy(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 250:\n",
        "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
        "                   %(epoch+1, num_epochs, batch_idx, \n",
        "                     len(train_loader), cost))\n",
        "\n",
        "    model = model.eval() # eval mode to prevent upd. batchnorm params during inference\n",
        "    with torch.set_grad_enabled(False): # save memory during inference\n",
        "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
        "              epoch+1, num_epochs, \n",
        "              compute_accuracy(model, train_loader)))\n",
        "\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "    \n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1JBvlC4ywyU",
        "outputId": "735f6c85-25ce-4190-e214-9c2601695fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 92.16%\n"
          ]
        }
      ],
      "source": [
        "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPaZRhGkywyV"
      },
      "source": [
        "## ResNet with convolutional blocks for resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aMo28g9ywyV"
      },
      "source": [
        "The following code implements the residual blocks with skip connections such that the input passed via the shortcut matches is resized to dimensions of the main path's output. Such a residual block is illustrated below:\n",
        "\n",
        "![](https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L14/2-resnet-ex/resnet-ex-1-2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTvvYxDjywyW"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\" Helper Class\"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        \n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.block = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=channels[0],\n",
        "                                out_channels=channels[1],\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(2, 2),\n",
        "                                padding=1),\n",
        "                torch.nn.BatchNorm2d(channels[1]),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "                torch.nn.Conv2d(in_channels=channels[1],\n",
        "                                out_channels=channels[2],\n",
        "                                kernel_size=(1, 1),\n",
        "                                stride=(1, 1),\n",
        "                                padding=0),   \n",
        "                torch.nn.BatchNorm2d(channels[2])\n",
        "        )\n",
        "\n",
        "        self.shortcut = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=channels[0],\n",
        "                                out_channels=channels[2],\n",
        "                                kernel_size=(1, 1),\n",
        "                                stride=(2, 2),\n",
        "                                padding=0),\n",
        "                torch.nn.BatchNorm2d(channels[2])\n",
        "        )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        \n",
        "        block = self.block(x)\n",
        "        shortcut = self.shortcut(x)    \n",
        "        x = torch.nn.functional.relu(block+shortcut)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkIys5lgywyW"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "### MODEL\n",
        "##########################\n",
        "\n",
        "\n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        self.residual_block_1 = ResidualBlock(channels=[1, 4, 8])\n",
        "        self.residual_block_2 = ResidualBlock(channels=[8, 16, 32])\n",
        "    \n",
        "        self.linear_1 = torch.nn.Linear(7*7*32, num_classes)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.residual_block_1(x)\n",
        "        out = self.residual_block_2(out)\n",
        "         \n",
        "        logits = self.linear_1(out.view(-1, 7*7*32))\n",
        "        return logits\n",
        "\n",
        "    \n",
        "torch.manual_seed(random_seed)\n",
        "model = ConvNet(num_classes=num_classes)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAwpLm2ywyX"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqfJ1jI5ywyX",
        "outputId": "55fc05d3-9fce-4150-9e56-1a74b8486798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/010 | Batch 000/468 | Cost: 2.4548\n",
            "Epoch: 001/010 | Batch 050/468 | Cost: 0.2955\n",
            "Epoch: 001/010 | Batch 100/468 | Cost: 0.1596\n",
            "Epoch: 001/010 | Batch 150/468 | Cost: 0.1105\n",
            "Epoch: 001/010 | Batch 200/468 | Cost: 0.1173\n",
            "Epoch: 001/010 | Batch 250/468 | Cost: 0.1747\n",
            "Epoch: 001/010 | Batch 300/468 | Cost: 0.0986\n",
            "Epoch: 001/010 | Batch 350/468 | Cost: 0.1371\n",
            "Epoch: 001/010 | Batch 400/468 | Cost: 0.1583\n",
            "Epoch: 001/010 | Batch 450/468 | Cost: 0.1172\n",
            "Epoch: 001/010 training accuracy: 97.29%\n",
            "Epoch: 002/010 | Batch 000/468 | Cost: 0.1137\n",
            "Epoch: 002/010 | Batch 050/468 | Cost: 0.0812\n",
            "Epoch: 002/010 | Batch 100/468 | Cost: 0.0268\n",
            "Epoch: 002/010 | Batch 150/468 | Cost: 0.0891\n",
            "Epoch: 002/010 | Batch 200/468 | Cost: 0.0918\n",
            "Epoch: 002/010 | Batch 250/468 | Cost: 0.0894\n",
            "Epoch: 002/010 | Batch 300/468 | Cost: 0.0487\n",
            "Epoch: 002/010 | Batch 350/468 | Cost: 0.0395\n",
            "Epoch: 002/010 | Batch 400/468 | Cost: 0.0729\n",
            "Epoch: 002/010 | Batch 450/468 | Cost: 0.0465\n",
            "Epoch: 002/010 training accuracy: 97.92%\n",
            "Epoch: 003/010 | Batch 000/468 | Cost: 0.0624\n",
            "Epoch: 003/010 | Batch 050/468 | Cost: 0.0483\n",
            "Epoch: 003/010 | Batch 100/468 | Cost: 0.0574\n",
            "Epoch: 003/010 | Batch 150/468 | Cost: 0.0765\n",
            "Epoch: 003/010 | Batch 200/468 | Cost: 0.1354\n",
            "Epoch: 003/010 | Batch 250/468 | Cost: 0.1235\n",
            "Epoch: 003/010 | Batch 300/468 | Cost: 0.1017\n",
            "Epoch: 003/010 | Batch 350/468 | Cost: 0.0681\n",
            "Epoch: 003/010 | Batch 400/468 | Cost: 0.0438\n",
            "Epoch: 003/010 | Batch 450/468 | Cost: 0.1454\n",
            "Epoch: 003/010 training accuracy: 98.13%\n",
            "Epoch: 004/010 | Batch 000/468 | Cost: 0.0770\n",
            "Epoch: 004/010 | Batch 050/468 | Cost: 0.0204\n",
            "Epoch: 004/010 | Batch 100/468 | Cost: 0.0570\n",
            "Epoch: 004/010 | Batch 150/468 | Cost: 0.0614\n",
            "Epoch: 004/010 | Batch 200/468 | Cost: 0.0572\n",
            "Epoch: 004/010 | Batch 250/468 | Cost: 0.0353\n",
            "Epoch: 004/010 | Batch 300/468 | Cost: 0.0270\n",
            "Epoch: 004/010 | Batch 350/468 | Cost: 0.0930\n",
            "Epoch: 004/010 | Batch 400/468 | Cost: 0.0832\n",
            "Epoch: 004/010 | Batch 450/468 | Cost: 0.0512\n",
            "Epoch: 004/010 training accuracy: 98.57%\n",
            "Epoch: 005/010 | Batch 000/468 | Cost: 0.0429\n",
            "Epoch: 005/010 | Batch 050/468 | Cost: 0.0750\n",
            "Epoch: 005/010 | Batch 100/468 | Cost: 0.0065\n",
            "Epoch: 005/010 | Batch 150/468 | Cost: 0.0870\n",
            "Epoch: 005/010 | Batch 200/468 | Cost: 0.0320\n",
            "Epoch: 005/010 | Batch 250/468 | Cost: 0.0463\n",
            "Epoch: 005/010 | Batch 300/468 | Cost: 0.0611\n",
            "Epoch: 005/010 | Batch 350/468 | Cost: 0.0231\n",
            "Epoch: 005/010 | Batch 400/468 | Cost: 0.0404\n",
            "Epoch: 005/010 | Batch 450/468 | Cost: 0.0195\n",
            "Epoch: 005/010 training accuracy: 98.85%\n",
            "Epoch: 006/010 | Batch 000/468 | Cost: 0.0530\n",
            "Epoch: 006/010 | Batch 050/468 | Cost: 0.0614\n",
            "Epoch: 006/010 | Batch 100/468 | Cost: 0.0070\n",
            "Epoch: 006/010 | Batch 150/468 | Cost: 0.0147\n",
            "Epoch: 006/010 | Batch 200/468 | Cost: 0.1086\n",
            "Epoch: 006/010 | Batch 250/468 | Cost: 0.0107\n",
            "Epoch: 006/010 | Batch 300/468 | Cost: 0.0102\n",
            "Epoch: 006/010 | Batch 350/468 | Cost: 0.0190\n",
            "Epoch: 006/010 | Batch 400/468 | Cost: 0.1027\n",
            "Epoch: 006/010 | Batch 450/468 | Cost: 0.0590\n",
            "Epoch: 006/010 training accuracy: 99.03%\n",
            "Epoch: 007/010 | Batch 000/468 | Cost: 0.0061\n",
            "Epoch: 007/010 | Batch 050/468 | Cost: 0.0112\n",
            "Epoch: 007/010 | Batch 100/468 | Cost: 0.0361\n",
            "Epoch: 007/010 | Batch 150/468 | Cost: 0.0968\n",
            "Epoch: 007/010 | Batch 200/468 | Cost: 0.0246\n",
            "Epoch: 007/010 | Batch 250/468 | Cost: 0.0144\n",
            "Epoch: 007/010 | Batch 300/468 | Cost: 0.0238\n",
            "Epoch: 007/010 | Batch 350/468 | Cost: 0.0086\n",
            "Epoch: 007/010 | Batch 400/468 | Cost: 0.0220\n",
            "Epoch: 007/010 | Batch 450/468 | Cost: 0.0672\n",
            "Epoch: 007/010 training accuracy: 99.04%\n",
            "Epoch: 008/010 | Batch 000/468 | Cost: 0.0551\n",
            "Epoch: 008/010 | Batch 050/468 | Cost: 0.0156\n",
            "Epoch: 008/010 | Batch 100/468 | Cost: 0.0025\n",
            "Epoch: 008/010 | Batch 150/468 | Cost: 0.0116\n",
            "Epoch: 008/010 | Batch 200/468 | Cost: 0.0093\n",
            "Epoch: 008/010 | Batch 250/468 | Cost: 0.0275\n",
            "Epoch: 008/010 | Batch 300/468 | Cost: 0.0581\n",
            "Epoch: 008/010 | Batch 350/468 | Cost: 0.0211\n",
            "Epoch: 008/010 | Batch 400/468 | Cost: 0.0224\n",
            "Epoch: 008/010 | Batch 450/468 | Cost: 0.0520\n",
            "Epoch: 008/010 training accuracy: 99.19%\n",
            "Epoch: 009/010 | Batch 000/468 | Cost: 0.0218\n",
            "Epoch: 009/010 | Batch 050/468 | Cost: 0.0742\n",
            "Epoch: 009/010 | Batch 100/468 | Cost: 0.0200\n",
            "Epoch: 009/010 | Batch 150/468 | Cost: 0.1219\n",
            "Epoch: 009/010 | Batch 200/468 | Cost: 0.0438\n",
            "Epoch: 009/010 | Batch 250/468 | Cost: 0.0195\n",
            "Epoch: 009/010 | Batch 300/468 | Cost: 0.0356\n",
            "Epoch: 009/010 | Batch 350/468 | Cost: 0.0959\n",
            "Epoch: 009/010 | Batch 400/468 | Cost: 0.0272\n",
            "Epoch: 009/010 | Batch 450/468 | Cost: 0.1140\n",
            "Epoch: 009/010 training accuracy: 99.09%\n",
            "Epoch: 010/010 | Batch 000/468 | Cost: 0.0308\n",
            "Epoch: 010/010 | Batch 050/468 | Cost: 0.0333\n",
            "Epoch: 010/010 | Batch 100/468 | Cost: 0.0286\n",
            "Epoch: 010/010 | Batch 150/468 | Cost: 0.0202\n",
            "Epoch: 010/010 | Batch 200/468 | Cost: 0.0193\n",
            "Epoch: 010/010 | Batch 250/468 | Cost: 0.0057\n",
            "Epoch: 010/010 | Batch 300/468 | Cost: 0.0331\n",
            "Epoch: 010/010 | Batch 350/468 | Cost: 0.0200\n",
            "Epoch: 010/010 | Batch 400/468 | Cost: 0.0588\n",
            "Epoch: 010/010 | Batch 450/468 | Cost: 0.0202\n",
            "Epoch: 010/010 training accuracy: 99.07%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model = model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "        \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "            \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(features)\n",
        "        cost = torch.nn.functional.cross_entropy(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
        "                   %(epoch+1, num_epochs, batch_idx, \n",
        "                     len(train_dataset)//batch_size, cost))\n",
        "\n",
        "    model = model.eval() # eval mode to prevent upd. batchnorm params during inference\n",
        "    with torch.set_grad_enabled(False): # save memory during inference\n",
        "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
        "              epoch+1, num_epochs, \n",
        "              compute_accuracy(model, train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUkVvlakywyX",
        "outputId": "609f1592-1e8e-4876-b6c1-3feb9af9be81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 98.13%\n"
          ]
        }
      ],
      "source": [
        "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}